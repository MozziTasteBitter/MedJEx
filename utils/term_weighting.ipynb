{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a73d99fa-6d5d-410e-a60a-8b6d0bb250fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0b4f6a-80ba-4e92-9a93-4001f7e9fb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4091565-0392-4a1a-a810-0464ab04817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordfreq\n",
    "from wordfreq import word_frequency, zipf_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8b1588-bda0-451f-bf96-be59604316ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59142370-ecd0-4084-96ce-330ce89da90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc3c64-ac93-4f06-a0b9-47c71e58837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = [{'sentid': 3145019, \n",
    "                   'ylabels': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'S', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'E', 'O', 'B', 'I', 'I', 'E', 'O', 'O', 'B', 'I', 'E', 'B', 'I', 'I', 'I', 'E', 'O', 'B', 'I', 'I', 'I', 'E', 'O', 'O', 'S', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], \n",
    "                   'yids': [4, 4, 4, 4, 4, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4, 0, 2, 4, 0, 1, 1, 2, 4, 4, 0, 1, 2, 0, 1, 1, 1, 2, 4, 0, 1, 1, 1, 2, 4, 4, 3, 4, 4, 4, 4, 4, 4, 4], 'text': 'The left lower extremity runoff reveals mild to moderate disease in the SFA and 2-vessel runoff with an occluded posterior tibial artery that reconstitutes at the level of the ankle . 4 .', \n",
    "                   'words': ['The', 'left', 'lower', 'extremity', 'runoff', 'reveals', 'mild', 'to', 'moderate', 'disease', 'in', 'the', 'SFA', 'and', '2-vessel', 'runoff', 'with', 'an', 'occluded', 'posterior', 'tibial', 'artery', 'that', 'reconstitutes', 'at', 'the', 'level', 'of', 'the', 'ankle', '.', '4', '.'], \n",
    "                   'tokens': ['[CLS]', 'The', 'left', 'lower', 'ex', '##tre', '##mity', 'runoff', 'reveals', 'mild', 'to', 'moderate', 'disease', 'in', 'the', 'SF', '##A', 'and', '2', '-', 'vessel', 'runoff', 'with', 'an', 'o', '##cc', '##luded', 'posterior', 't', '##ibi', '##al', 'artery', 'that', 're', '##con', '##st', '##it', '##utes', 'at', 'the', 'level', 'of', 'the', 'ankle', '.', '4', '.', '[SEP]'], \n",
    "                   'token_ids': [101, 1109, 1286, 2211, 4252, 7877, 15455, 21814, 7189, 10496, 1106, 8828, 3653, 1107, 1103, 18659, 1592, 1105, 123, 118, 5832, 21814, 1114, 1126, 184, 19515, 27567, 16530, 189, 21883, 1348, 18593, 1115, 1231, 7235, 2050, 2875, 20311, 1120, 1103, 1634, 1104, 1103, 10845, 119, 125, 119, 102], \n",
    "                   'entities': [{'uid': 11086, 'start_token': 40, 'end_token': 41, 'term': 'vertebral level', 'term_frequency': 2.81}, {'uid': 11087, 'start_token': 24, 'end_token': 27, 'term': 'occlude', 'term_frequency': 1.8}, {'uid': 11088, 'start_token': 7, 'end_token': 8, 'term': 'runoff', 'term_frequency': 3.53}, {'uid': 11089, 'start_token': 27, 'end_token': 32, 'term': 'posterior tibial artery', 'term_frequency': 2.35}, {'uid': 11090, 'start_token': 33, 'end_token': 38, 'term': 'reconstitutes', 'term_frequency': 1.36}, {'uid': 11092, 'start_token': 18, 'end_token': 22, 'term': '2-vessel runoff', 'term_frequency': 3.46}, {'uid': 11093, 'start_token': 15, 'end_token': 17, 'term': 'superficial femoral artery', 'term_frequency': 2.75}], \n",
    "                   'word_index2token_indices': {0: [1], 1: [2], 2: [3], 3: [4, 5, 6], 4: [7], 5: [8], 6: [9], 7: [10], 8: [11], 9: [12], 10: [13], 11: [14], 12: [15, 16], 13: [17], 14: [18, 19, 20], 15: [21], 16: [22], 17: [23], 18: [24, 25, 26], 19: [27], 20: [28, 29, 30], 21: [31], 22: [32], 23: [33, 34, 35, 36, 37], 24: [38], 25: [39], 26: [40], 27: [41], 28: [42], 29: [43], 30: [44], 31: [45], 32: [46]}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e624d1a-5bd2-4683-b14e-465ab648fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermWeighting(object):\n",
    "    def __init__(self):\n",
    "        self.weight_calculator\n",
    "        \n",
    "    def get_weight(self, term):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fcecc-f548-4506-8c06-c7a022ee3528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TermFrequency(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_weight(self, term, mode='zipf', lang = 'en'):\n",
    "        # word freq = word probability\n",
    "        if mode == 'word':\n",
    "            term_weight = word_frequency(term, lang)\n",
    "        \n",
    "        # Zipf freq = log10(bilion * word_frequency)\n",
    "        elif mode == 'zipf':\n",
    "            term_weight = zipf_frequency(term, lang)\n",
    "        \n",
    "        else:\n",
    "            term_weight = None\n",
    "        \n",
    "        return term_weight\n",
    "        \n",
    "    def get_weights(self, processed_data, target_entities = 'entities'):\n",
    "        temp_processed_data = []\n",
    "        for processed_sentence in tqdm(processed_data):\n",
    "            token_ids = processed_sentence['token_ids']; \n",
    "            entities = processed_sentence[target_entities]\n",
    "            \n",
    "            temp_entities = []\n",
    "            for entity in entities:\n",
    "                term = entity['term']\n",
    "                entity['term_frequency'] = self.get_weight(term, mode = 'zipf', lang = 'en')\n",
    "                temp_entities.append(entity)\n",
    "            processed_sentence[target_entities] = temp_entities\n",
    "            temp_processed_data.append(processed_sentence)\n",
    "        return temp_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daf6628-b47c-467f-bb53-7383a04c94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class for get BERT-based term weighting\n",
    "__init__(model_name = \"\"): 1) initialize tokenizer and model, 2) set [MASK]ing func\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class MLM_weight(object):\n",
    "    def __init__(self, model_name, mode='mask'):\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        \n",
    "        pretrained_model_config = AutoConfig.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.mask_id = self.tokenizer.mask_token_id\n",
    "        \n",
    "        \n",
    "        # self.preprocessor = self.set_preprocessor()\n",
    "        \n",
    "        self.model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # set mode\n",
    "        # mode == (mask, flat, pll)\n",
    "        self.mode = mode\n",
    "        \n",
    "    def mask_sentence_generator(self,token_ids, entities):\n",
    "        mask_id = self.mask_id\n",
    "        \n",
    "        masked_token_sentences = []\n",
    "        for entity in entities:\n",
    "            tokens = copy.copy(token_ids)\n",
    "            start_token = entity['start_token']; end_token = entity['end_token']; uid = entity['uid']\n",
    "            if self.mode == 'mask':\n",
    "                masked_sentences = tokens[:start_token]  + [mask_id] * (end_token - start_token) + tokens[end_token:]\n",
    "            else:\n",
    "                masked_sentences = tokens\n",
    "            # entity['masked_sentences'] = masked_sentences\n",
    "            masked_token_sentences.append(masked_sentences)\n",
    "            \n",
    "        \n",
    "        return masked_token_sentences    \n",
    "        \n",
    "    def get_masked_weights(self, masked_token_sentences, entities, golden_tokens):\n",
    "        def make_inputs(masked_token_sentences, golden_tokens):\n",
    "            device = self.device\n",
    "            # print(masked_token_sentences)\n",
    "            token_len = len(masked_token_sentences[0])\n",
    "            \n",
    "            input_ids = [masked_token_sentence for masked_token_sentence in masked_token_sentences]\n",
    "            attention_mask = [[1]*token_len for masked_token_sentence in masked_token_sentences] \n",
    "            token_type_ids = [[0]*token_len for masked_token_sentence in masked_token_sentences] \n",
    "            labels = [golden_tokens for masked_token_sentence in masked_token_sentences]\n",
    "            \n",
    "            \n",
    "            input_ids = torch.tensor(input_ids).to(device)\n",
    "            attention_mask = torch.tensor(attention_mask).to(device)\n",
    "            token_type_ids = torch.tensor(token_type_ids).to(device)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "            \n",
    "            return input_ids, attention_mask, token_type_ids, labels\n",
    "        \n",
    "        def nll(masked_logits, golden, spans):\n",
    "            start_token, end_token = spans\n",
    "            # masked_logits.cpu().data; golden.cpu().data; \n",
    "            # masked_logits = masked_logits[start_token:end_token]\n",
    "            # golden = golden[start_token:end_token]\n",
    "            softmax = torch.nn.Softmax(dim=-1)\n",
    "\n",
    "            \n",
    "            masked_probs = softmax(masked_logits)\n",
    "            \n",
    "           \n",
    "            golden = [[gold] for gold in golden]\n",
    "            golden = torch.tensor(golden)\n",
    "            # print(golden)\n",
    "            masked_answer_probs = torch.gather(masked_probs, 1, golden)[start_token: end_token]\n",
    "            \n",
    "            \n",
    "            # print(self.tokenizer.decode(torch.squeeze(golden[start_token: end_token], -1)))\n",
    "            # print(torch.log(masked_answer_probs))\n",
    "            # print()\n",
    "            \n",
    "            masked_negative_log_likelihood = -1 * torch.mean(torch.log(masked_answer_probs))\n",
    "            # masked_negative_log_likelihood.item() \n",
    "            \n",
    "            return masked_negative_log_likelihood.item()\n",
    "            \n",
    "        model = self.model\n",
    "        \n",
    "        input_ids, attention_mask, token_type_ids, labels = make_inputs(masked_token_sentences, golden_tokens)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask=attention_mask,labels=labels, token_type_ids = token_type_ids)\n",
    "        \n",
    "        loss = outputs[0]; logits = outputs[1].cpu().data;\n",
    "        \n",
    "        weights = []\n",
    "        # print(logits)\n",
    "        for masked_token_sentence, masked_logits in zip(entities,logits):\n",
    "            start_token = masked_token_sentence['start_token']; end_token = masked_token_sentence['end_token']\n",
    "            masked_nll = nll(masked_logits, golden_tokens, (start_token, end_token))\n",
    "            weight = masked_nll\n",
    "            weights.append(weight)\n",
    "        del(input_ids); del(attention_mask); del(token_type_ids); del(labels); del(logits)\n",
    "        # print(outputs)\n",
    "        # print(outputs[1].shape)\n",
    "        # print(self.tokenizer.decode(golden_tokens))\n",
    "        # print(self.tokenizer.decode(input_ids[1]))\n",
    "        \n",
    "        return weights\n",
    "    \n",
    "    def get_weights(self, processed_data, target_entities = 'UMLS_entities'):\n",
    "        temp_processed_data = []\n",
    "        for processed_sentence in tqdm(processed_data):\n",
    "            \n",
    "            token_ids = processed_sentence['token_ids']; \n",
    "            entities = processed_sentence[target_entities];\n",
    "            if entities:\n",
    "                # temp_entities = [copy.copy(entity )for entity in entities]\n",
    "                masked_token_sentences = self.mask_sentence_generator(token_ids, entities)\n",
    "\n",
    "                weights = self.get_masked_weights(masked_token_sentences, entities, token_ids)\n",
    "\n",
    "                temp_entities = []\n",
    "                for weight, entity in zip(weights, entities):\n",
    "                    entity['MLM_weight'] = weight\n",
    "                    temp_entities.append(entity)\n",
    "                processed_sentence[target_entities] = temp_entities\n",
    "                temp_processed_data.append(processed_sentence)\n",
    "            else: \n",
    "                temp_processed_data.append(processed_sentence)\n",
    "        return temp_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08381a05-72ff-483c-a12c-e1b33843e8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_weighting = TermFrequency()\n",
    "MLM_weighting.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e018b4-da13-46c2-8df6-22bfca075f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLM_weighting = MLM_weight(\"bert-base-cased\", mode='mask')\n",
    "MLM_weighting.get_weights(processed_data, target_entities='entities')[0]['entities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218dd7bb-b1b6-4edb-a558-837536dd4eee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e7a7484-4168-4eb6-bcef-c9eeddc295d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.75e-07"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_frequency('diuretics', lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00580f91-467c-47ad-8a42-c6f7fd57e1ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
