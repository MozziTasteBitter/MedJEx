{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3aafd1-630b-4f14-ba1f-57b4d144afe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "import random\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\" \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "\n",
    "from pprint import pprint as pprint\n",
    "from typing import List, Optional\n",
    "from copy import copy\n",
    "import urllib.request\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbeb82e-45cc-4a15-bfbc-50ff43170e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import sklearn\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c474b-d401-4613-bd08-63a9c7a3d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce575b18-9834-421d-b679-129ef5d6244c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68da7cf-874a-41ff-b152-cfa380c56304",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: import transformers; \n",
    "except: \n",
    "  !pip install transformers\n",
    "  import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import RobertaModel, RobertaTokenizer, RobertaPreTrainedModel, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForTokenClassification, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2be7b69-48bb-466c-960a-7923304cafa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e63cf5-937a-48a4-a508-a3675da8dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loader.loader import load_file, load_data, load_ner_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab526e0-861c-4a2a-acf1-b91a0cfd8482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sequence_labeler import SequenceLabeler\n",
    "from utils.scorers import F1\n",
    "from utils.term_weighting import TermFrequency, MLM_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce17f54-6fa4-47a1-9fe7-f30548c2e8fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93104b73-22fa-4bf3-81ff-649c13a9f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.models import BERT_MLP, BertForTokenClassification, BertCRFs,BertMLPCRFs, EarlyFusionBertMLPCRF, EarlyAndLateFusionBertMLPCRF, EarlyAndLateFusionrobertaMLPCRF\n",
    "from models.models import EarlyFusionRobertaMLPCRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866b5e1-9ecd-4b9c-834b-64d42bdf2252",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d5f1c2-c9f8-4c5e-a489-ac47fdb940a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b34ae1-2baf-4382-b513-a03a7ad25b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: import medspacy;\n",
    "except:\n",
    "    !pip install medspacy\n",
    "#from quickumls import QuickUMLS\n",
    "# QuickUMLS_PATH = '/mnt/nfs/scratch1/sunjaekwon/UMLS/QuickUMLS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6b4196-2e5d-4860-b1b5-7483c9020894",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.MedCAT import MedCAT_wrapper\n",
    "from quickumls import QuickUMLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593efbc6-67ab-48fa-8695-946d396e3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.normalization import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f7a01c-079f-4b79-b46b-6802681b3308",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_seed = 0\n",
    "random.seed(manual_seed)\n",
    "torch.manual_seed(manual_seed)\n",
    "np.random.seed(manual_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a6ce74-943f-409b-9f2a-b632b716edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.MedCAT import MedCAT_wrapper\n",
    "from quickumls import QuickUMLS\n",
    "\n",
    "#QuickUMLS_PATH = '/mnt/nfs/scratch1/sunjaekwon/UMLS/QuickUMLS'\n",
    "#UMLS_MedCAT_PATH = '/home/sunjae/Utils/MedCAT/umls_sm_wstatus_2021_oct/umls_sm_wstatus_2021_oct'\n",
    "\n",
    "UMLS_MedCAT_PATH = # ADD YOUR MEDCAT PATH\n",
    "\n",
    "MedCAT = True\n",
    "\n",
    "if MedCAT:\n",
    "    UMLS_matcher = MedCAT_wrapper(UMLS_MedCAT_PATH)\n",
    "else:\n",
    "    UMLS_matcher = QuickUMLS(QuickUMLS_PATH, overlapping_criteria=\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d55e01-5e67-4c4d-9893-c0dcbc581472",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933d61b-7a6f-4a7f-8ffe-7db8b2c80390",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PATH = './data'\n",
    "DATA_PATH = os.path.join(DEFAULT_PATH, '')\n",
    "NOTE_AID_PATH = os.path.join(DATA_PATH, 'sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ccf210-7d95-49ad-a16a-e3eae4dce33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "\n",
    "wiki = True\n",
    "normalization = True\n",
    "normalization_type = 'min_max'\n",
    "MAX_TOKEN_LEN = 512\n",
    "Binary_flag = False\n",
    "TF_flag = False\n",
    "MLM_flag = False\n",
    "additional_feature = False\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ca4e19-2b8a-4aff-ab76-d16869b90c99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#BINARY_PATH = 'results/%s_3_5e-05_0.1_%s_True_min_max_True_True_False_False/model.pth'%(MODEL_NAME, str(wiki))\n",
    "if not os.path.isfile('results/MedJEx.pth'):\n",
    "    urllib.request.urlretrieve(\"https://huggingface.co/Mozzi/MedJEx/resolve/main/model.pth\", \"results/MedJEx.pth\")\n",
    "BINARY_PATH = 'results/MedJEx.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4176e43b-1d43-4715-beda-fc530768265a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed174a-4565-44b5-8d8f-9c87d8e0ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print('load files!')\n",
    "note_aid_data_dict = load_file(NOTE_AID_PATH)\n",
    "\n",
    "BIOES_labeler = SequenceLabeler(labeling_scheme='BIOES',longest_labeling_flag=True)\n",
    "UMLS_labeler = SequenceLabeler(labeling_scheme='BIOES',longest_labeling_flag=True)\n",
    "\n",
    "PROCESSED_DICT_FILE = 'data/%s_processed_data_dict.pkl'%MODEL_NAME\n",
    "if os.path.exists(PROCESSED_DICT_FILE):\n",
    "    processed_data_dict = pickle.load(open(PROCESSED_DICT_FILE,'rb'))\n",
    "else:\n",
    "    processed_data_dict = load_data(note_aid_data_dict, tokenizer, BIOES_labeler, UMLS_matcher = UMLS_matcher, UMLS_labeler=UMLS_labeler)\n",
    "    pickle.dump(processed_data_dict, open(PROCESSED_DICT_FILE,'wb'))\n",
    "    \n",
    "#print(processed_data_dict[list(processed_data_dict.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5805d38-4dee-435c-aa91-118ab001d31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eebdd5-5379-4805-93ec-1a5eb80c2658",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_splits = {}\n",
    "\n",
    "PROCESSED_DICT_FILE = 'data/%s_data_split_dict.pkl'%MODEL_NAME\n",
    "if os.path.exists(PROCESSED_DICT_FILE):\n",
    "    data_splits = pickle.load(open(PROCESSED_DICT_FILE,'rb'))\n",
    "    \n",
    "else:\n",
    "    data_splits = {}\n",
    "    train_data_portion = 1.0\n",
    "    keys = list(processed_data_dict.keys())\n",
    "    random.shuffle(keys)\n",
    "    \n",
    "    train_keys = keys[:int(0.8*len(keys))]\n",
    "    dev_keys = keys[int(0.8*len(keys)):int(0.90*len(keys))]\n",
    "    test_keys = keys[int(0.9*len(keys)):]\n",
    "    \n",
    "    train_split = [processed_data_dict[key] for key in train_keys if len(processed_data_dict[key]['token_ids']) < MAX_TOKEN_LEN]\n",
    "    train_split = train_split[:int(len(train_split)*train_data_portion)]\n",
    "    dev_split = [processed_data_dict[key] for key in dev_keys if len(processed_data_dict[key]['token_ids'])  < MAX_TOKEN_LEN]\n",
    "    test_split = [processed_data_dict[key] for key in test_keys if len(processed_data_dict[key]['token_ids'])  < MAX_TOKEN_LEN]\n",
    "    \n",
    "    TF_weighting = TermFrequency()\n",
    "    train_split = TF_weighting.get_weights(processed_data=train_split, target_entities='entities')\n",
    "    dev_split = TF_weighting.get_weights(processed_data=dev_split, target_entities='entities')\n",
    "    test_split = TF_weighting.get_weights(processed_data=test_split, target_entities='entities')\n",
    "    train_split = TF_weighting.get_weights(processed_data=train_split, target_entities='UMLS_concepts')\n",
    "    dev_split = TF_weighting.get_weights(processed_data=dev_split, target_entities='UMLS_concepts')\n",
    "    test_split = TF_weighting.get_weights(processed_data=test_split, target_entities='UMLS_concepts')\n",
    "    \n",
    "    MLM_weighting = MLM_weight(MODEL_NAME)\n",
    "    train_split = MLM_weighting.get_weights(processed_data=train_split, target_entities='entities')\n",
    "    dev_split = MLM_weighting.get_weights(processed_data=dev_split, target_entities='entities')\n",
    "    test_split = MLM_weighting.get_weights(processed_data=test_split, target_entities='entities')\n",
    "    train_split = MLM_weighting.get_weights(processed_data=train_split, target_entities='UMLS_concepts')\n",
    "    dev_split = MLM_weighting.get_weights(processed_data=dev_split, target_entities='UMLS_concepts')\n",
    "    test_split = MLM_weighting.get_weights(processed_data=test_split, target_entities='UMLS_concepts')\n",
    "    \n",
    "    data_splits['train_split'] = train_split\n",
    "    data_splits['dev_split'] = dev_split\n",
    "    data_splits['test_split'] = test_split\n",
    "    \n",
    "    pickle.dump(data_splits, open(PROCESSED_DICT_FILE,'wb'))\n",
    "    \n",
    "    import json\n",
    "    train_keys = [data['sentid'] for data in train_split]\n",
    "    test_keys = [data['sentid'] for data in test_split]\n",
    "    dev_keys = [data['sentid'] for data in dev_split]\n",
    "\n",
    "    data_split = {'train_sents': train_keys,\n",
    "                  'test_sents': test_keys,\n",
    "                  'dev_sents': dev_keys}\n",
    "\n",
    "    str_data_split = json.dumps(data_split)\n",
    "\n",
    "    SPLIT_DATA_PATH = os.path.join(DEFAULT_PATH,\"train_dev_test_sent_nums.jsonl\")\n",
    "    fin = open(SPLIT_DATA_PATH,'w')\n",
    "    fin.write(str_data_split)\n",
    "    fin.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48687d42-c5cd-416d-bdf1-d6f99c849604",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = data_splits['train_split']\n",
    "dev_split = data_splits['dev_split']\n",
    "test_split = data_splits['test_split'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c343bab-9856-4792-a8b4-a8ce2a23515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_split))\n",
    "print(len(dev_split))\n",
    "print(len(test_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4fc26-1032-41db-a0fa-3ee350a9b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(data_splits, normalizer, target_entities):\n",
    "    for data_index, data_split in enumerate(data_splits):\n",
    "        concepts = data_split[target_entities]\n",
    "        for concept_index, concept in enumerate(concepts):\n",
    "            data_splits[data_index][target_entities][concept_index]['term_frequency'] = normalizer.normalizer['term_frequency'].get_normalized_results(concept['term_frequency'])\n",
    "            data_splits[data_index][target_entities][concept_index]['MLM_weight'] = normalizer.normalizer['MLM_weight'].get_normalized_results(concept['MLM_weight'])\n",
    "    return data_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ce8a25-2ae4-4d6e-ba5d-90d8f901386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JargonTerm(Dataset):\n",
    "    def __init__(self, JargonTerm_data, tokenizer, labeler, MAX_TOKEN_LEN=256, Binary_flag = False, TF_flag = False, MLM_flag = False, UMLS_lablers = None):\n",
    "        self.data = JargonTerm_data\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "        self.outter_id = labeler.label2id['O']\n",
    "        self.MAX_TOKEN_LEN = MAX_TOKEN_LEN\n",
    "        \n",
    "        \n",
    "        self.TF_flag = TF_flag\n",
    "        self.MLM_flag = MLM_flag\n",
    "        self.Binary_flag = Binary_flag\n",
    "        \n",
    "        self.additional_feature_flag = False\n",
    "        if Binary_flag or TF_flag or MLM_flag:\n",
    "            self.additional_feature_flag = True\n",
    "        #     self.num_of_additional_features = len(UMLS_lablers.label2id)\n",
    "        # else:\n",
    "        #     self.additional_feature_flag = False\n",
    "        #     self.num_of_additional_features = 0\n",
    "        \n",
    "        \n",
    "        self.num_of_additional_features = 0\n",
    "        self.num_of_binary_features = 0\n",
    "        self.num_of_weighted_features = 0\n",
    "\n",
    "        self.flags = [self.TF_flag, self.MLM_flag]\n",
    "        \n",
    "        if True in self.flags:\n",
    "            self.num_of_additional_features += len(UMLS_lablers.label2id)\n",
    "        self.num_of_additional_features += sum([len(UMLS_lablers.label2id) if flag else 0 for flag in self.flags])\n",
    "        \n",
    "        if self.Binary_flag:\n",
    "            self.num_of_binary_features += len(UMLS_lablers.label2id)\n",
    "        if True in self.flags:\n",
    "            self.num_of_weighted_features += sum([len(UMLS_lablers.label2id) if flag else 0 for flag in self.flags])\n",
    "        \n",
    "        \n",
    "        #self.num_of_UMLS_labels = len(UMLS_lablers.label2id)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        def _padding(loaded_data, MAX_TOKEN_LEN):\n",
    "            token_len = len(loaded_data['token_ids'])\n",
    "            margin_len = self.MAX_TOKEN_LEN - token_len\n",
    "            \n",
    "            input_ids = loaded_data['token_ids'] + [self.pad_id] * margin_len\n",
    "            label_ids = loaded_data['yids'] + [self.outter_id] * margin_len\n",
    "            attention_mask = [1] * token_len + [0] * margin_len\n",
    "            token_type_ids = [0] * token_len + [0] * margin_len\n",
    "            \n",
    "            return input_ids, attention_mask, token_type_ids, label_ids\n",
    "        \n",
    "        def _truncating(loaded_data, MAX_TOKEN_LEN):\n",
    "            input_ids = loaded_data['token_ids'][:MAX_TOKEN_LEN]\n",
    "            label_ids = loaded_data['yids'][:MAX_TOKEN_LEN] \n",
    "            attention_mask = [1] * MAX_TOKEN_LEN\n",
    "            token_type_ids = [0] * MAX_TOKEN_LEN\n",
    "            \n",
    "            return input_ids, attention_mask, token_type_ids, label_ids\n",
    "        \n",
    "        def _term_features(loaded_data, MAX_TOKEN_LEN):\n",
    "            token_len = len(loaded_data['token_ids'])\n",
    "            margin_len = self.MAX_TOKEN_LEN - token_len\n",
    "            \n",
    "            concepts = loaded_data['UMLS_concepts']\n",
    "            # UMLS_yids = loaded_data['UMLS_yids']\n",
    "            \n",
    "            # UMLS_bin_representation = loaded_data['UMLS_bin_representation']\n",
    "            # UMLS_bin_dim = UMLS_bin_representation.shape[1]\n",
    "            # UMLS_bin_representation_margin = np.zeros((margin_len, UMLS_bin_dim))\n",
    "            # UMLS_bin_representation = np.concatenate((UMLS_bin_representation, UMLS_bin_representation_margin), axis = 0)\n",
    "            def binary_feature_map(concepts, tokens, feature=None):\n",
    "                labelings = []\n",
    "                for index, concept in enumerate(concepts):\n",
    "                    # set as a first concept among the candidates\n",
    "                    #concept = concept[0]\n",
    "                    cui = concept['cui']; term = concept['term']; semtypes=concept['semtypes']\n",
    "                    start_token = concept['start_token']; end_token = concept['end_token'];\n",
    "\n",
    "                    if feature:\n",
    "                        weight = concept[feature]\n",
    "                    else:\n",
    "                        weight = 1.0\n",
    "                    \n",
    "                    labelings.append(((start_token, end_token, cui), weight))\n",
    "                \n",
    "                UMLS_bin_dim = len(UMLS_labeler.id2label)\n",
    "                token_len = len(tokens)\n",
    "                UMLS_bin_representation = np.zeros((self.MAX_TOKEN_LEN, UMLS_bin_dim))\n",
    "                \n",
    "                for token_index, representation in enumerate(UMLS_bin_representation):\n",
    "                    UMLS_bin_representation[token_index][UMLS_labeler.label2id['O']] = 1.0\n",
    "                \n",
    "                for labeling in labelings:\n",
    "                    weight = labeling[1]; labeling = labeling[0];\n",
    "                    UMLS_ylabels, UMLS_yids = UMLS_labeler.get_labels(tokens, [labeling])\n",
    "                    for token_id, yid in enumerate(UMLS_yids):\n",
    "                            if UMLS_labeler.label2id['O'] != yid and token_id < 512:\n",
    "                                UMLS_bin_representation[token_id][yid] = 1.0 #weight\n",
    "                                UMLS_bin_representation[token_id][UMLS_labeler.label2id['O']] = 0.0\n",
    "                \n",
    "                return UMLS_bin_representation\n",
    "            \n",
    "            def weighted_feature_map(concepts, tokens, feature=None):\n",
    "                labelings = []\n",
    "                for index, concept in enumerate(concepts):\n",
    "                    # set as a first concept among the candidates\n",
    "                    #concept = concept[0]\n",
    "                    cui = concept['cui']; term = concept['term']; semtypes=concept['semtypes']\n",
    "                    start_token = concept['start_token']; end_token = concept['end_token'];\n",
    "\n",
    "                    if feature:\n",
    "                        weight = concept[feature]\n",
    "                    else:\n",
    "                        weight = 1.0\n",
    "                    \n",
    "                    labelings.append(((start_token, end_token, cui), weight))\n",
    "                \n",
    "                UMLS_bin_dim = len(UMLS_labeler.id2label)\n",
    "                token_len = len(tokens)\n",
    "                UMLS_weighted_representation = np.zeros((self.MAX_TOKEN_LEN, UMLS_bin_dim))\n",
    "                \n",
    "                # for token_index, representation in enumerate(UMLS_bin_representation):\n",
    "                #     UMLS_bin_representation[token_index][UMLS_labeler.label2id['O']] = 1.0\n",
    "                \n",
    "                for labeling in labelings:\n",
    "                    weight = labeling[1]; labeling = labeling[0];\n",
    "                    UMLS_ylabels, UMLS_yids = UMLS_labeler.get_labels(tokens, [labeling])\n",
    "                    for token_id, yid in enumerate(UMLS_yids):\n",
    "                            if UMLS_labeler.label2id['O'] != yid:\n",
    "                                UMLS_weighted_representation[token_id][yid] = weight\n",
    "                                UMLS_weighted_representation[token_id][UMLS_labeler.label2id['O']] = 0.0\n",
    "                \n",
    "                return UMLS_weighted_representation\n",
    "                \n",
    "            \n",
    "            \n",
    "            \n",
    "            additional_features = {}\n",
    "            if self.TF_flag or self.MLM_flag or self.Binary_flag:\n",
    "                bin_weights = binary_feature_map(concepts, loaded_data['token_ids'], feature = 'term_frequency')\n",
    "                additional_features['bin_weights'] = torch.tensor(bin_weights, dtype = torch.float32)\n",
    "            if self.TF_flag:\n",
    "                    TF_weights = weighted_feature_map(concepts, loaded_data['token_ids'], feature = 'term_frequency')\n",
    "                    additional_features['TF_weights'] = torch.tensor(TF_weights, dtype = torch.float32)\n",
    "            if self.MLM_flag:\n",
    "                    MLM_weights = weighted_feature_map(concepts, loaded_data['token_ids'], feature = 'MLM_weight')\n",
    "                    additional_features['MLM_weights'] = torch.tensor(MLM_weights, dtype = torch.float32)\n",
    "#             additional_features = {}\n",
    "            \n",
    "#             # initialize addiatinal feature lists\n",
    "#             # UMLS_flags = np.zeros((MAX_TOKEN_LEN, self.num_of_UMLS_labels))\n",
    "            \n",
    "#             if self.TF_flag:\n",
    "#                 TF_weights = np.zeros((MAX_TOKEN_LEN, 1))\n",
    "#             if self.MLM_flag:\n",
    "#                 MLM_weights = np.zeros((MAX_TOKEN_LEN, 1))\n",
    "                \n",
    "            \n",
    "#             for concept in concepts:\n",
    "#                 start_token = concept['start_token']; end_token = concept['end_token']\n",
    "#                 MLM_weight = concept['MLM_weight']; term_frequency = concept['term_frequency']\n",
    "                \n",
    "#                 for token_index in range(start_token, end_token):\n",
    "#                     # UMLS_flags[token_index] = 1 \n",
    "#                     if self.TF_flag:\n",
    "#                         TF_weights[token_index][0] = .0 #term_frequency \n",
    "#                     if self.MLM_flag:\n",
    "#                         MLM_weights[token_index][0] =  .0 #MLM_weight\n",
    "#             # for token_index, UMLS_yid in enumerate(UMLS_yids):\n",
    "#             #     UMLS_flags[token_index][UMLS_yid] = 1.0\n",
    "                    \n",
    "#             # additional_features['UMLS_flags'] = torch.tensor(UMLS_flags, dtype = torch.float32)\n",
    "#             additional_features['UMLS_flags'] = torch.tensor(UMLS_bin_representation, dtype = torch.float32)\n",
    "            \n",
    "                \n",
    "            \n",
    "            return additional_features\n",
    "        \n",
    "        loaded_data = self.data[idx]\n",
    "        if len(loaded_data['token_ids']) < self.MAX_TOKEN_LEN:\n",
    "            processed_data = _padding(loaded_data, self.MAX_TOKEN_LEN)\n",
    "        else:\n",
    "            processed_data = _truncating(loaded_data, self.MAX_TOKEN_LEN)\n",
    "        input_ids, attention_mask, token_type_ids, labels = processed_data\n",
    "        \n",
    "        \n",
    "        \n",
    "        item = {}\n",
    "        item['input_ids'] = torch.tensor(input_ids);\n",
    "        item['attention_mask'] = torch.tensor(attention_mask)\n",
    "        item['token_type_ids'] = torch.tensor(token_type_ids)\n",
    "        item['labels'] = torch.tensor(labels)\n",
    "        item['sentid'] = loaded_data['sentid']\n",
    "        \n",
    "        if self.additional_feature_flag:\n",
    "            additional_features = _term_features(loaded_data, self.MAX_TOKEN_LEN)\n",
    "            #item['UMLS_flags'] = additional_features['UMLS_flags']\n",
    "            item['bin_weights'] = additional_features['bin_weights']\n",
    "            if self.TF_flag:\n",
    "                item['TF_weights'] = additional_features['TF_weights']\n",
    "            if self.MLM_flag:\n",
    "                item['MLM_weights'] = additional_features['MLM_weights']\n",
    "            \n",
    "        else: item['additional_features'] = {}\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a800b2d-1e61-48b2-bbc6-2b62dd4336b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if normalization:\n",
    "    normalizer = Normalizer(train_split, normalization_type = normalization_type)\n",
    "    \n",
    "    train_split = data_normalization(train_split, normalizer, target_entities = 'UMLS_concepts')\n",
    "    dev_split = data_normalization(dev_split, normalizer, target_entities = 'UMLS_concepts')\n",
    "    test_split = data_normalization(test_split, normalizer, target_entities = 'UMLS_concepts')\n",
    "    \n",
    "    train_split = data_normalization(train_split, normalizer, target_entities = 'entities')\n",
    "    dev_split = data_normalization(dev_split, normalizer, target_entities = 'entities')\n",
    "    test_split = data_normalization(test_split, normalizer, target_entities = 'entities')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56f3fba-7474-4972-9a58-6d5b990dd267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65783cb4-a65a-4d41-a5fd-30faeefd3d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36507d9e-69bc-4de1-86e1-a1dd01553d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8811d32-a3ac-4ad1-9a07-0e7ae6abd6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class evaluation_model(object):\n",
    "    \n",
    "    def __init__(self, PATH, data_splits, tokenizer, BIOES_labeler, UMLS_labeler):\n",
    "        model_data = torch.load(PATH)\n",
    "        self.model_data  = model_data\n",
    "        \n",
    "        train_split = data_splits['train_split']\n",
    "        dev_split = data_splits['dev_split']\n",
    "        test_split = data_splits['test_split'] \n",
    "        \n",
    "        normalization = model_data['normalization']\n",
    "        normalization_type = model_data['normalization_type']\n",
    "        # normalization =True\n",
    "        # normalization_type = 'min_max'\n",
    "        if normalization:\n",
    "            \n",
    "            normalizer = Normalizer(train_split, normalization_type = normalization_type)\n",
    "            train_split = data_normalization(train_split, normalizer, target_entities = 'UMLS_concepts')\n",
    "            dev_split = data_normalization(dev_split, normalizer, target_entities = 'UMLS_concepts')\n",
    "            test_split = data_normalization(test_split, normalizer, target_entities = 'UMLS_concepts')\n",
    "            \n",
    "            \n",
    "        MAX_TOKEN_LEN = 128\n",
    "        \n",
    "        \n",
    "        Binary_flag = model_data['Binary_flag']\n",
    "        TF_flag = model_data['TF_flag']\n",
    "        MLM_flag = model_data['MLM_flag']\n",
    "        additional_feature = model_data['additional_feature']\n",
    "        \n",
    "        train_dataset = JargonTerm(train_split, tokenizer, BIOES_labeler, MAX_TOKEN_LEN, Binary_flag, TF_flag, MLM_flag, UMLS_labeler)\n",
    "        dev_dataset = JargonTerm(dev_split, tokenizer, BIOES_labeler, MAX_TOKEN_LEN, Binary_flag, TF_flag, MLM_flag, UMLS_labeler)\n",
    "        test_dataset = JargonTerm(test_split, tokenizer, BIOES_labeler, MAX_TOKEN_LEN, Binary_flag, TF_flag, MLM_flag, UMLS_labeler)\n",
    "        \n",
    "        num_of_additional_features = {}\n",
    "        num_of_additional_features['num_of_binary_features'] = train_dataset.num_of_binary_features\n",
    "        num_of_additional_features['num_of_weighted_features'] = train_dataset.num_of_weighted_features\n",
    "\n",
    "        if not additional_feature:\n",
    "            num_of_additional_features = None\n",
    "        else:\n",
    "            if not TF_flag and not MLM_flag:\n",
    "                num_of_additional_features['num_of_weighted_features'] = 0\n",
    "        \n",
    "        batch_size = model_data['batch_size']\n",
    "        \n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size = batch_size)\n",
    "        dev_loader = DataLoader(dev_dataset, batch_size = batch_size)\n",
    "        \n",
    "        pretrained_model_config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=BIOES_labeler.num_of_label)\n",
    "        # model = BERT_MLP.from_pretrained(MODEL_NAME, config=pretrained_model_config, num_of_additional_features = num_of_additional_features)\n",
    "        if 'roberta' in  MODEL_NAME:\n",
    "            self.model = EarlyAndLateFusionrobertaMLPCRF.from_pretrained(MODEL_NAME, config=pretrained_model_config, num_of_additional_features = num_of_additional_features)\n",
    "        else:\n",
    "            self.model = EarlyAndLateFusionBertMLPCRF.from_pretrained(MODEL_NAME, config=pretrained_model_config, num_of_additional_features = num_of_additional_features)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.model.load_state_dict(model_data['model_state_dict'], strict=False)\n",
    "        # model.to(device)\n",
    "        # device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        # torch.cuda.current_device()\n",
    "        \n",
    "        self.Binary_flag = Binary_flag\n",
    "        self.TF_flag = TF_flag\n",
    "        self.MLM_flag = MLM_flag\n",
    "        self.additional_feature = additional_feature\n",
    "\n",
    "        self.dev_split = dev_split\n",
    "        self.test_split = test_split\n",
    "        \n",
    "        self.dev_loader = dev_loader\n",
    "        self.test_loader = test_loader\n",
    "        \n",
    "        \n",
    "        self.batch_size = batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59722a08-641b-45e9-a978-7e5795552f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARY_model = evaluation_model(BINARY_PATH, data_splits, tokenizer, BIOES_labeler, UMLS_labeler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02f716b-310e-4535-aab2-029646cf9b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673cdac9-a698-46a6-8959-771d8268fa16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08b0649-1a92-4e6f-9350-d0a17b85de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "mednlp = medspacy.load()\n",
    "\n",
    "def sentencify(mednlp, text):\n",
    "    sents = [sent.text.strip() for sent in mednlp(text).sents if sent.text]\n",
    "        \n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf9e3a-aa14-4619-819c-f68fc37e9c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1000a6-789c-4c55-8e42-ff85cb0fb0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sents(sents):\n",
    "    \n",
    "    fout = open('temp/temp.tsv','w')\n",
    "    \n",
    "    \n",
    "    for sent in sents:\n",
    "        words = word_tokenize(sent)\n",
    "        \n",
    "        for word in words:\n",
    "            fout.write(\"%s\\t%s\\n\"%(word, 'O'))\n",
    "        fout.write('\\n')\n",
    "        \n",
    "    fout.close()\n",
    "#     note_aid_data_dict = {}\n",
    "    \n",
    "#     sent_dict = {}\n",
    "#     for i, sent in enumerate(sents):\n",
    "#         sent_dict[str(i)] = sent\n",
    "    \n",
    "    note_aid_data_dict = load_ner_file('temp/temp.tsv')\n",
    "    \n",
    "    return note_aid_data_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d506d1d-b279-49d5-8731-a6ca051393aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIOES_decode(tokens, labels, tokenizer = None):\n",
    "    # Output [{entity_type=\"\", text = \"\", entity_token_span=(start,end)}]\n",
    "    assert len(tokens) == len(labels), \"the length of tokens and labels should be same.\"\n",
    "    entity_list = []\n",
    "\n",
    "    inner = False\n",
    "    for i, (token, label) in enumerate(zip(tokens, labels)):\n",
    "      label_type = label[0]\n",
    "    \n",
    "      #print(token, label)\n",
    "      if len(label) > 0: \n",
    "        entity_type = label[2:]\n",
    "      else:\n",
    "        entity_type = \"\"\n",
    "      \n",
    "      # Type 1, not inner \n",
    "      #print(label_type)\n",
    "      if not inner:\n",
    "        if label_type == 'B':\n",
    "          inner = (i, entity_type)\n",
    "        elif label_type == 'S':\n",
    "          entity_list.append((i, i+1, entity_type))\n",
    "        else: continue;\n",
    "      else:\n",
    "        if label_type == 'B' or label_type == 'S':\n",
    "          inner = False; continue;\n",
    "        elif inner[1] != entity_type:\n",
    "          inner = False; continue;\n",
    "        elif label_type == 'E':\n",
    "          start = inner[0]; end = i+1\n",
    "          inner = False; entity_list.append((start, end, entity_type))\n",
    "        else:\n",
    "          continue\n",
    "    #print(entity_list)\n",
    "    entities = []\n",
    "    for entity in entity_list:\n",
    "      start = entity[0]; end = entity[1]; entity_type = entity[2]\n",
    "      if not tokenizer:\n",
    "        text = \" \".join(tokens[start: end])\n",
    "      else:\n",
    "        text = tokenizer.convert_tokens_to_string(tokens[start: end])\n",
    "      entities.append({ 'entity_type': entity_type,\n",
    "                        'entity_token_span': (start, end),\n",
    "                        'start_token': start,\n",
    "                        'end_token': end,\n",
    "                        'text': text})\n",
    "\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af623c-9bb6-4805-9514-8c2cc9921c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def MedJEx(input_text):\n",
    "#     sents = sentencify(mednlp, input_text)\n",
    "#     note_aid_data_dict = load_sents(sents)\n",
    "#     processed_data_dict = load_data(note_aid_data_dict, tokenizer, BIOES_labeler, UMLS_matcher = UMLS_matcher, UMLS_labeler=UMLS_labeler)\n",
    "    \n",
    "#     keys = list(processed_data_dict.keys())\n",
    "#     test_keys = keys\n",
    "\n",
    "#     test_split = [processed_data_dict[key] for key in test_keys if len(processed_data_dict[key]['token_ids'])]\n",
    "\n",
    "#     TF_weighting = TermFrequency()\n",
    "#     test_split = TF_weighting.get_weights(processed_data=test_split, target_entities='entities')\n",
    "#     test_split = TF_weighting.get_weights(processed_data=test_split, target_entities='UMLS_concepts')\n",
    "\n",
    "#     MLM_weighting = MLM_weight(MODEL_NAME)\n",
    "#     test_split = MLM_weighting.get_weights(processed_data=test_split, target_entities='entities')\n",
    "#     test_split = MLM_weighting.get_weights(processed_data=test_split, target_entities='UMLS_concepts')\n",
    "    \n",
    "    \n",
    "#     temp_model = BINARY_model\n",
    "    \n",
    "    \n",
    "    \n",
    "#     test_golden = []\n",
    "#     test_preds = []\n",
    "#     f1s = []\n",
    "#     medical_jargons = set()\n",
    "\n",
    "#     model = temp_model.model\n",
    "#     model.to(device)\n",
    "#     torch.cuda.current_device()\n",
    "#     dev_loader = temp_model.dev_loader\n",
    "\n",
    "#     TF_flag = temp_model.TF_flag\n",
    "#     MLM_flag = temp_model.MLM_flag\n",
    "#     Binary_flag = temp_model.Binary_flag\n",
    "    \n",
    "#     if normalization:\n",
    "#         normalizer = Normalizer(test_split, normalization_type = normalization_type)\n",
    "\n",
    "#         test_split = data_normalization(test_split, normalizer, target_entities = 'UMLS_concepts')\n",
    "#         test_split = data_normalization(test_split, normalizer, target_entities = 'entities')\n",
    "#     test_dataset = JargonTerm(test_split, tokenizer, BIOES_labeler, MAX_TOKEN_LEN, Binary_flag, TF_flag, MLM_flag, UMLS_labeler)\n",
    "#     test_loader = DataLoader(test_dataset, batch_size = 1)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#     for batch in tqdm(test_loader):\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         token_type_ids = batch['token_type_ids'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "\n",
    "#         additional_features = {}\n",
    "#         if Binary_flag or TF_flag or MLM_flag:\n",
    "#             # UMLS_flags = batch['UMLS_flags']\n",
    "#             weighted_features = torch.tensor([])\n",
    "#             bin_weights = batch['bin_weights']\n",
    "#             additional_features['binary_features'] = bin_weights.to(device)\n",
    "\n",
    "#             if TF_flag:\n",
    "#                 TF_weights = batch['TF_weights']\n",
    "#                 weighted_features = torch.cat((weighted_features, TF_weights), dim = -1)\n",
    "#             if MLM_flag:\n",
    "#                 MLM_weights = batch['MLM_weights']\n",
    "#                 weighted_features = torch.cat((weighted_features, MLM_weights), dim = -1)\n",
    "#             additional_features['weighted_features'] = weighted_features.to(device)\n",
    "#             #print(additional_features)\n",
    "#         # print(additional_features.shape)\n",
    "#         # print(type(additional_features))\n",
    "#         sentids = batch['sentid']\n",
    "\n",
    "\n",
    "#         #optimizer.zero_grad()\n",
    "#         outputs = model(input_ids, \n",
    "#                         attention_mask=attention_mask, \n",
    "#                         labels=labels, \n",
    "#                         token_type_ids = token_type_ids, \n",
    "#                         additional_features = additional_features)\n",
    "#         loss = outputs[0]; preds = outputs[1]#.cpu().detach().numpy()\n",
    "#         # dev_losses.append(loss.cpu().data)\n",
    "\n",
    "#         # probs = softmax(logits).cpu().data\n",
    "#         # preds = torch.argmax(probs, dim=-1).cpu().detach().numpy()\n",
    "\n",
    "#         labels = [BIOES_labeler.id2label[pred] for pred in preds[0]]\n",
    "\n",
    "#         input_ids = [tokenizer.convert_ids_to_tokens([input_id])[0] for input_id in input_ids[0].cpu().data.numpy()[:len(labels)]]\n",
    "\n",
    "#         # print(input_ids)\n",
    "#         # print(labels)\n",
    "#         # print(len(labels))\n",
    "#         jargons = BIOES_decode(input_ids, labels, tokenizer)\n",
    "\n",
    "#         for jargon in jargons:\n",
    "#             #print(jargon)\n",
    "#             medical_jargons.add(jargon['text'])\n",
    "\n",
    "#     #del(model)\n",
    "    \n",
    "#     return medical_jargons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea78f2-4c25-4c10-a05b-32b6561ffd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedJEx(object):\n",
    "    def __init__(self, BINARY_model, device):\n",
    "        #temp_model = BINARY_model\n",
    "        self.BINARY_model = BINARY_model\n",
    "        #self.model = BINARY_model.to(device)\n",
    "        \n",
    "        self.model = BINARY_model.model.to(device)\n",
    "        #self.model = \n",
    "        #torch.cuda.current_device()\n",
    "        self.dev_loader = BINARY_model.dev_loader\n",
    "        \n",
    "        self.MLM_weighting = MLM_weight(MODEL_NAME)\n",
    "        self.TF_weighting = TermFrequency()\n",
    "        \n",
    "    def predict(self, input_text, bz = 4, ignore_MLM_weighting = False):\n",
    "        def _sentencify(mednlp, text):\n",
    "            sents = [sent.text.strip() for sent in mednlp(text).sents if sent.text]\n",
    "\n",
    "            return sents\n",
    "        torch.cuda.empty_cache()\n",
    "        sents = _sentencify(mednlp, input_text)\n",
    "        note_aid_data_dict = load_sents(sents)\n",
    "        processed_data_dict = load_data(note_aid_data_dict, tokenizer, BIOES_labeler, UMLS_matcher = UMLS_matcher, UMLS_labeler=UMLS_labeler)\n",
    "\n",
    "        keys = list(processed_data_dict.keys())\n",
    "        test_keys = keys\n",
    "\n",
    "        test_split = [processed_data_dict[key] for key in test_keys if len(processed_data_dict[key]['token_ids'])]\n",
    "\n",
    "        \n",
    "        TF_weighting = self.TF_weighting\n",
    "        test_split = TF_weighting.get_weights(processed_data=test_split, target_entities='entities')\n",
    "        test_split = TF_weighting.get_weights(processed_data=test_split, target_entities='UMLS_concepts')\n",
    "\n",
    "        MLM_weighting = self.MLM_weighting\n",
    "        test_split = MLM_weighting.get_weights(processed_data=test_split, target_entities='entities', ignore = ignore_MLM_weighting)\n",
    "        test_split = MLM_weighting.get_weights(processed_data=test_split, target_entities='UMLS_concepts', ignore = ignore_MLM_weighting)\n",
    "\n",
    "\n",
    "        #temp_model = BINARY_model\n",
    "        model = self.model\n",
    "        dev_loader = self.dev_loader\n",
    "\n",
    "        test_golden = []\n",
    "        test_preds = []\n",
    "        f1s = []\n",
    "        medical_jargons = set()\n",
    "\n",
    "        # model = temp_model.model\n",
    "        # model.to(device)\n",
    "        # torch.cuda.current_device()\n",
    "        # dev_loader = temp_model.dev_loader\n",
    "\n",
    "        TF_flag = self.BINARY_model.TF_flag\n",
    "        MLM_flag = self.BINARY_model.MLM_flag\n",
    "        Binary_flag = self.BINARY_model.Binary_flag\n",
    "\n",
    "        if normalization:\n",
    "            normalizer = Normalizer(test_split, normalization_type = normalization_type)\n",
    "\n",
    "            test_split = data_normalization(test_split, normalizer, target_entities = 'UMLS_concepts')\n",
    "            test_split = data_normalization(test_split, normalizer, target_entities = 'entities')\n",
    "        test_dataset = JargonTerm(test_split, tokenizer, BIOES_labeler, MAX_TOKEN_LEN, Binary_flag, TF_flag, MLM_flag, UMLS_labeler)\n",
    "        test_loader = DataLoader(test_dataset, batch_size = bz)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for batch in test_loader:\n",
    "            torch.cuda.empty_cache()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            additional_features = {}\n",
    "            if Binary_flag or TF_flag or MLM_flag:\n",
    "                # UMLS_flags = batch['UMLS_flags']\n",
    "                weighted_features = torch.tensor([])\n",
    "                bin_weights = batch['bin_weights']\n",
    "                additional_features['binary_features'] = bin_weights.to(device)\n",
    "\n",
    "                if TF_flag:\n",
    "                    TF_weights = batch['TF_weights']\n",
    "                    weighted_features = torch.cat((weighted_features, TF_weights), dim = -1)\n",
    "                if MLM_flag:\n",
    "                    MLM_weights = batch['MLM_weights']\n",
    "                    weighted_features = torch.cat((weighted_features, MLM_weights), dim = -1)\n",
    "                additional_features['weighted_features'] = weighted_features.to(device)\n",
    "                #print(additional_features)\n",
    "            # print(additional_features.shape)\n",
    "            # print(type(additional_features))\n",
    "            sentids = batch['sentid']\n",
    "\n",
    "            \n",
    "            #optimizer.zero_grad()\n",
    "            outputs = model(input_ids, \n",
    "                            attention_mask=attention_mask, \n",
    "                            labels=labels, \n",
    "                            token_type_ids = token_type_ids, \n",
    "                            additional_features = additional_features)\n",
    "            loss = outputs[0]; preds = outputs[1]#.cpu().detach().numpy()\n",
    "            # dev_losses.append(loss.cpu().data)\n",
    "            #print(preds)\n",
    "\n",
    "            # probs = softmax(logits).cpu().data\n",
    "            # preds = torch.argmax(probs, dim=-1).cpu().detach().numpy()\n",
    "            numpy_input_ids = input_ids.cpu().data.numpy()\n",
    "            for batch_index, pred in enumerate(preds):\n",
    "                labels = [BIOES_labeler.id2label[p] for p in pred]\n",
    "                #print(input_ids[batch_index].cpu())\n",
    "                input_ids = [tokenizer.convert_ids_to_tokens([input_id])[0] for input_id in numpy_input_ids[batch_index][:len(labels)]]\n",
    "\n",
    "                # print(input_ids)\n",
    "                # print(labels)\n",
    "                # print(len(labels))\n",
    "                jargons = BIOES_decode(input_ids, labels, tokenizer)\n",
    "\n",
    "                for jargon in jargons:\n",
    "                    #print(jargon)\n",
    "                    medical_jargons.add(jargon['text'])\n",
    "\n",
    "        #del(model)\n",
    "\n",
    "        return medical_jargons\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a02b2d-7296-4bee-a68c-08769db507ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# print(torch.cuda.memory_allocated())\n",
    "# print(torch.cuda.memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ff4de0-76f1-4fcb-b68c-3c9bf0ba3092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c895820-9994-404a-96d3-b23bca9376c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "medjex = MedJEx(BINARY_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75602758-00d3-49b2-89a8-8e258fb11a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# medjex.predict(input_text, bz = 16, ignore_MLM_weighting=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec4924-4c69-46c4-9f27-098fa791bfe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e36188-527f-4fb5-ac1c-4ad25dd2e548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# meta_dataset = []\n",
    "# filename = \"/mnt/nfs_home/sunjae/Projects/test/comprehnotes.jsonl\"\n",
    "# with open(filename, \"r\") as file:\n",
    "#     lines = json.loads(file.read())\n",
    "#     meta_dataset = lines\n",
    "#     # for line in file:\n",
    "#     #     meta_dataset.append(json.loads(line))\n",
    "# filename = \"/mnt/nfs_home/sunjae/Projects/test/comprehnotes_with_medjex.jsonl\"\n",
    "# meta_dataset_with_medjex = []\n",
    "# with open(filename, \"w\") as file:\n",
    "#     for data in tqdm(meta_dataset):\n",
    "#         question = data['question']\n",
    "#         medical_jargons = medjex.predict(question, bz = 16, ignore_MLM_weighting=True)\n",
    "#         # print(question)\n",
    "#         # print(medical_jargons)\n",
    "#         data['medjex_phrases'] = list(medical_jargons)\n",
    "#         file.write(json.dumps(data)+'\\n')\n",
    "#     file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa59a4f-585d-4fc9-9823-5e4c7b934c75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "medjex.predict('Fertilization begins when sperm binds to the corona radiata of the egg. Once the sperm enters the cytoplasm, a cortical reaction occurs which prevents other sperm from entering the oocyte. The oocyte then undergoes an important reaction. What is the next reaction that is necessary for fertilization to continue?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b071a64-f0e4-4564-a6b7-669f8907bc23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d271747-1972-49b6-b626-35f7a9190833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filename = \"/mnt/nfs_home/sunjae/Projects/test/biolaysumm.json\"\n",
    "# def data_loader(file_path):\n",
    "#     with open(file_path) as json_file:\n",
    "#         data = json.load(json_file)\n",
    "#         doc_texts = []\n",
    "#         for doc in data:\n",
    "#             text = \"\"\n",
    "#             for section in doc[\"sections\"]:\n",
    "#                 text += \" \".join(section)\n",
    "#                 text += \"\\n\\n\"\n",
    "#             doc_texts.append(text)\n",
    "#     return doc_texts, data\n",
    "\n",
    "# biolaysumm, data = data_loader(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25a3a3-2fc6-41c7-acd5-9615a5dc1fe7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3321cd6-4f8f-4176-a4c3-e97fb0206dc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# for index, note in enumerate(biolaysumm):\n",
    "#     lines = sent_tokenize(note)\n",
    "#     #text = note['sections']\n",
    "    \n",
    "    \n",
    "#     medical_jargons = medjex.predict(note, bz = 16, ignore_MLM_weighting=True)\n",
    "#     data[index]['medjex_jargons'] = medical_jargons\n",
    "#     print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c757f7f4-7039-45f5-9214-b192c3508fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fout = open(\"/mnt/nfs_home/sunjae/Projects/test/sample_biolaysumm.json\",'w')\n",
    "# #json.dump(data, fout)\n",
    "# fout.write(str(data))\n",
    "# fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1bf3f-8308-4c6a-94e3-d8af66188802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# with open(filename, \"r\") as file:\n",
    "#     text = file.read()\n",
    "\n",
    "# lines = sent_tokenize(text)\n",
    "# medical_jargons = medjex.predict(text, bz = 16, ignore_MLM_weighting=True)\n",
    "# xa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159eac1d-ecbc-4ea1-89e5-0c230bf755d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a055ab46-c648-4fee-845d-b5fdd7ce95ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fout = open(\"/mnt/nfs_home/sunjae/Projects/test/sample_biolaysumm.txt\",'w')\n",
    "# for jargon in medical_jargons:\n",
    "#     fout.write(jargon + '\\n')\n",
    "# fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57befd3-a5ee-473d-8e74-017aa99e41b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206a8003-9f4f-4664-8aba-b1c39d8c23a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f516b95-aee1-4e07-a4dc-e5de8596e5f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eea2be-b79b-432e-a50b-87d94bae23db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
